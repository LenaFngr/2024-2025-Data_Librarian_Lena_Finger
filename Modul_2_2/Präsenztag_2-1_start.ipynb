{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe Teilmodul 2.1\n",
    "## Auswahl Ihrer Daten\n",
    "Suchen Sie auf bibsonomy mit einem selbst gewählten Suchbegriff nach Publikationen. Auf Bibsonomy lassen sich die Ergebnisse einer Keyword-Suche (z.B. \"Bibliothek\") in einer Vielzahl verschiedener Formate exportieren (Limit ggf. vorher auf 1000 erhöhen) und lokal speichern. Geben Sie dafür einen Suchbegriff oben rechts in die Maske ein und bestätigen Sie die Eingabe. Rechts neben der Überschrift der rechten Spalte (\"publications\"/\"Publikationen\") klicken Sie ganz rechte auf das Symbol mit dem Pfeil nach rechts, der aus einem Kasten zeigt (\"export options for displayed posts\"/\"Exportieren für angezeigte Einträge\"), dann auf mehr/more. Wählen Sie ZUERST bei posts/Einträge \"1000\" aus, und wählen sie dann in dem Format drop-Down-Menü fast ganz unten JSON aus. Es öffnet sich die JSON Datei in ihrem Browser.  Sie können die URL wie im Tutorial über das requests Modul in Python öffnen.\n",
    "\n",
    "Alternativ können Sie die Datei auch lokal speichern. Dafür rechtsklicken Sie irgendwo, wählen Sie \"save as...\"/\"speichern unter...\" und speichern Sie die Datei unter einem passenden Dateinamen mit der Endung \".json\".\n",
    "\n",
    "Für eine Nutzung im zweiten Modulteil ist ein großer Datensatz sinnvoll. Fügen Sie daher die Ergebnisse mehrerer Suchen mit Pandas in einem Datensatz zusammenführen.\n",
    "Fügen sie ihren Datensätzen dabei den Suchterm hinzu, der zu den Ergebnissen geführt hat. Tipp: Informationen dazu finden Sie unter \"Daten Verändern und Hinzufügen\".\n",
    "\n",
    "Nutzen sie dabei Python (vorzugsweise in einem Jupyter Notebook) mithilfe von Pandas. Sollten Sie Probleme haben, schauen Sie im Pandas Tutorial unter \"JSON einlesen\" oder bitten Sie Fabian Haak im Moodle Austauschforum um Hilfe.\n",
    "Legen Sie Ihr Notebook bzw. Python-Script mit den Lösungen in einem \"Modul_2-1\" Ordner im eigenen Repositorium auf GitHub ab. Fügen Sie mich als zusätzlichen Collaborator (GitHub-Name: HaakFab) hinzu, so kann ich besser Feedback oder Hilfestellungen geben.\n",
    "\n",
    "## Exportformat erstellen\n",
    "Diese Aufgabe dient als direkte Vorbereitung des nächsten Modulteils. Wir empfehlen, Pandas für die Bearbeitung der Aufgabe zu nutzen. \n",
    "\n",
    "### Datenaufbereitung\n",
    "Für den nächsten Teil dieses Modules möchten wir unseren Datensatz wieder als JSON-File ablegen. Zunächst möchten wir aber die Bibsonomy-Daten noch etwas zu bereinigen:\n",
    "- Filtern Sie die Daten so, dass nur noch Einträge des Typs \"Publication\" enthalten sind.\n",
    "- Überlegen Sie, welche Felder Sie möglicherweise umbenennen wollen, und welche Sie nicht benötigen. Benennen Sie entsprechend um und wählen Sie nur die Spalten, die für Sie sinnvolle Informationen enthalten. Mögliche Änderungen der Daten sind das Umbenennen von Felder zu eindeutigeren Bezeichnungen (wie beispielsweise im Pandas Tutorial \"label\" zu \"title\"). Filtern Sie ihre Daten jedoch nicht zu drastisch und entfernen Sie nicht zu viele Felder, damit im zweiten Modulteil noch genügend Informationen übrig bleiben.\n",
    "- Wenn sie mehrere Suchergebnisse zusammengeführt haben, ist es sinnvoll, Nachduplikaten zu überprüfen.\n",
    "- BONUS: Wie bei den Jareszahlen ist es durchaus sinnvoll, zu überprüfen, ob alle Einträge \"sauber\" vorliegen. Stellen sie dafür sicher, dass alle Einträge Informationen zu Titel, AutorInnen und Veröffentlichungsjahr enthalten und dass das Jahr eine sinnvolle Zahl ist!\n",
    "Daten Export\n",
    "\n",
    "### Erstellen Sie einen sauberen Export im JSON-Format, das von Solr akzeptiert wird.\n",
    "\n",
    "Für die Weiterverwendung ist nicht relevant, wie die einzelnen Felder benannt sind oder wie viele Felder es gibt, Es ist aber entscheidend, dass die äußere Form einer Liste von Dictionaries gewahrt wird. (HINWEIS BEI PROBLEMEN: Schauen Sie im Pandas Tutorial unter \"Export als JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 Ergebnisse für query artificial intelligence gefunden.\n",
      "2000 Ergebnisse für query AI gefunden.\n",
      "2000 Ergebnisse für query machine learning gefunden.\n",
      "1311 Ergebnisse für query language model gefunden.\n",
      "1410 Ergebnisse für query KI gefunden.\n",
      "1275 Ergebnisse für query künstliche intelligenz gefunden.\n",
      "295 Ergebnisse für query maschinelles lernen gefunden.\n",
      "30 Ergebnisse für query sprachmodell gefunden.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# zum Zusammenbauen der Bibsonomy URL brauchen wir alles, was vor und nach der Query kommt\n",
    "url_base = \"https://www.bibsonomy.org/json/search/\"\n",
    "url_attr = \"?items=1000\"\n",
    "\n",
    "def df_from_query(query):\n",
    "    # die URL; wir müssen Leerzeichen ersetzen mit \"%20\" nach ASCII Encoding guideline\n",
    "    url = url_base + query.replace(\" \", \"%20\") + url_attr\n",
    "    \n",
    "    result = requests.get(url)\n",
    "    json_data = result.json()\n",
    "    \n",
    "    # im Feld \"items\" stecken die relevanten Dateneinträge\n",
    "    dataframe = pd.DataFrame(json_data[\"items\"])\n",
    "\n",
    "    # wir filtern so, dass nur Publikationen bleiben\n",
    "    dataframe.loc[dataframe[\"type\"] == \"Publication\"] # dataframe = ... dadurch muss später nicht nach publications gefiltert werden\n",
    "\n",
    "    # wir möchten die query in die Daten eintragen \n",
    "    dataframe[\"query\"] = query\n",
    "    \n",
    "    # die Rückgabe der Methode nicht vergessen:\n",
    "    return dataframe\n",
    "\n",
    "# wir erstellen eine leere Liste, in die wir unsere Dataframes packen\n",
    "dataframes = []\n",
    "\n",
    "# eine Liste von Suchanfragen\n",
    "queries = [\"artificial intelligence\", \"AI\", \"machine learning\", \"language model\", \"KI\", \"künstliche intelligenz\", \"maschinelles lernen\", \"sprachmodell\"]\n",
    "\n",
    "# jetzt gehen wir die Liste von Suchanfragen durch und rufen unsere Funktion für jede Suchanfrage auf\n",
    "for query in queries:\n",
    "    df = df_from_query(query)\n",
    "    # schließlich fügen wir das DataFrame über append unserer Liste von Dataframes hinzu\n",
    "    dataframes.append(df)\n",
    "\n",
    "    # wie wir uns eine Statusausgabe schreiben würden:\n",
    "    # print(\"Ergebnisse für \", query , \" gefunden. Dataframe enthält \",len(df), \" Einträge.\")\n",
    "    \n",
    "    # mit einem f String lassen sich Variablen in Strings viel einfacher formatieren:\n",
    "    print(f\"{len(df)} Ergebnisse für query {query} gefunden.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leere zelle mit a erzeugen über der angesteuerten\n",
    "# mit b daruntr \n",
    "# escape geht von einer zelle im schreibmodus raus und markiert nur die zelle\n",
    "# enter geht rein zum schreiben\n",
    "# x löscht ganze zelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_conc = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Da die dataframes die selbe From (column titles sind identisch, wenn für alle vorhanden) haben, können wir die Dataframes über concat zusammenfügen\n",
    "data_conc = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtern Sie die Daten so, dass nur noch Einträge des Typs \"Publication\" enthalten sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_conc.loc[data_conc[\"type\"] == \"Publication\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[data_conc[\"type\"] == \"Publication\"] # true und false werte werden in funktion darüber ausgelesen, alle true sind in publications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Überlegen Sie, welche Felder Sie möglicherweise umbenennen wollen, und welche Sie nicht benötigen. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns.to_list() # alle spalten als liste ausgeben lassen, damit ist es übersichtlicher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selection = data[[\n",
    "    'id', \n",
    "    'label', \n",
    "    'tags'\n",
    "]] # weitere daten können aus gesamten datensatz ausgewählt werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benennen Sie entsprechend um und wählen Sie nur die Spalten, die für Sie sinnvolle Informationen enthalten. Mögliche Änderungen der Daten sind das Umbenennen von Felder zu eindeutigeren Bezeichnungen (wie beispielsweise im Pandas Tutorial \"label\" zu \"title\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selection = data_selection.rename(columns={\"id\":\"docno\", \"label\":\"title\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wenn sie mehrere Suchergebnisse zusammengeführt haben, ist es sinnvoll, nach Duplikaten zu überprüfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selection = data_selection.drop_duplicates(subset = \"title\", keep = \"first\") # keep = last, keep = false\n",
    "# dokumentationen zu pandas online nutzen\n",
    "len(data_selection) # zeigt nur die anzahl der zeilen, also lände des df an"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SABINE:\n",
    "- Sollte man Dublikate vor dem Merge entfernen lassen oder lieber danach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ansprechen einer zeile:\n",
    "# data.year ist das gleich wie data[\"year\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS: Wie bei den Jareszahlen ist es durchaus sinnvoll, zu überprüfen, ob alle Einträge \"sauber\" vorliegen. Stellen sie dafür sicher, dass alle Einträge Informationen zu Titel, AutorInnen und Veröffentlichungsjahr enthalten und dass das Jahr eine sinnvolle Zahl ist!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selection = data_selection.dropna(subset = [\"title\", \"authors\", \"year\"])\n",
    "# alle zeilen raus bei denen in einem der spalten kein wert ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selection.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_year(value): # ausgabe des jahreszahl soll ein check mit true und false sein\n",
    "    if len(value) == 4:\n",
    "        for character in value:\n",
    "            if character not in [str(a) for a in list(range(0,10))]: #wenn character nicht in liste enthalten ist\n",
    "                return False\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selection[\"year_checked\"] = data_selection.apply(lambda row: check_if_year(row[\"year\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selection = data_selection.loc[data_selection[\"year_checked\"] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wenn noch Zeit ist (sonst: wird nachgereicht!)\n",
    "\n",
    "#### LARA:\n",
    "- habe bei der Autoren-Spalte alle NaN Einträge rausgelöscht später aber festgestellt, dass ich noch solche Einträge habe: [?]\n",
    "Wie könnte ich diese rausfiltern und löschen sodass ich am Ende nur Namen in der Spalte habe?\n",
    "- in der Spalte \"language\" hatte ich Titel auf deutsch und englisch. Deutsche Titel hatten bspws. folgende Werte: dt, ger, german\n",
    "Wie kann ich diese Werte alle in \"deutsch\" umbenennen?\n",
    "\n",
    "\n",
    "\n",
    "1. Language Spalte säubern --> nur noch EN oder DE als Werte! Bei keiner angabe EN eintragen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gruppenaufgabe 1\n",
    "### Aufgabenbeschreibung\n",
    "Wir möchten nun zusätzlich zu den Bibsonomy Daten auch noch Daten aus einer weiteren Quelle zusammenführen, in diesem Fall Semantic Scholar.\n",
    "\n",
    "1. Lade \"bibsonomy.json\" und \"semantic_scholar.json\" in Pandas als ein Dataframe, füge eine Spalte \"source\" hinzu, in der du festhälst, woher die Suchergebnisse kommen  (also: bibsonomy oder semantic_scholar).\n",
    "2. Selektiere und benenne die Spalten so um, dass aus beiden Quellen Titel, Autoren, URL, Abstract, Keywords, Query (Suchanfrage) sowie das Jahr enthalten und gleich benannt sind.\n",
    "3. Stelle sicher, dass die Autoren gleich als Liste formatiert sind (also z. B. [Max Mustermann, Lieschen Müller] oder [Mustermann M., Müller L.]).\n",
    "4. Führe die Datensätze zusammen.\n",
    "5. Stelle sicher, dass alle Einträge für \"Jahr\" eine 4-stellige Zahl (int) sind.\n",
    "6. Entferne Duplikate (anhand der Spalte Title).\n",
    "7. Sorge dafür, dass es einen durchgängigen Index gibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten laden\n",
    "data_bib = json.load(open(\"bibsonomy.json\"))\n",
    "data_sem = json.load(open(\"semantic_scholar.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste in dataframe umwandeln\n",
    "df_bib = pd.DataFrame(data_bib) # schritt mit items als key muss nicht durchgeführt werden, wurde schon bereinigt\n",
    "df_sem = pd.DataFrame(data_sem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oder in einem schritt: \n",
    "df_bib = pd.read_json(\"bibsonomy.json\")\n",
    "df_sem = pd.read_json(\"semantic_scholar.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bib[\"source\"] = \"bibsonomy\"\n",
    "df_sem[\"source\"] = \"semantic_scholar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'tags', 'query', 'title', 'description', 'date', 'url', 'year',\n",
       "       'author', 'authors', 'text', 'pages', 'abstract', 'source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bib.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'paperId', 'externalIds', 'publicationVenue', 'url', 'title',\n",
       "       'abstract', 'venue', 'year', 'referenceCount', 'citationCount',\n",
       "       'influentialCitationCount', 'isOpenAccess', 'openAccessPdf',\n",
       "       'fieldsOfStudy', 'publicationTypes', 'publicationDate', 'journal',\n",
       "       'citationStyles', 'authors', 'searchQuery', 'source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sem.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bib_sel = df_bib[['query', 'title', 'date', 'url', 'year', 'author', 'abstract', 'source']] # erst auswahl treffen vor zusammenfügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bib_sel = df_bib_sel.rename(columns={\"author\":\"authors\"}) # teilweise namen ändernn sodass in beiden DF gleich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sem_sel = df_sem[['searchQuery', 'title', 'url', 'year', 'authors', 'abstract', 'source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sem_sel = df_sem_sel.rename(columns={\"searchQuery\":\"query\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'authorId': '121645690', 'name': 'Sébastien Bubeck'},\n",
       "  {'authorId': '143754359', 'name': 'Varun Chandrasekaran'},\n",
       "  {'authorId': '2315830', 'name': 'Ronen Eldan'},\n",
       "  {'authorId': '120962807', 'name': 'J. Gehrke'},\n",
       "  {'authorId': '2064595436', 'name': 'Eric Horvitz'},\n",
       "  {'authorId': '1783184', 'name': 'Ece Kamar'},\n",
       "  {'authorId': '2212084492', 'name': 'Peter Lee'},\n",
       "  {'authorId': '2109308930', 'name': 'Y. Lee'},\n",
       "  {'authorId': '152244300', 'name': 'Yuan-Fang Li'},\n",
       "  {'authorId': '23451726', 'name': 'Scott M. Lundberg'},\n",
       "  {'authorId': '40900039', 'name': 'Harsha Nori'},\n",
       "  {'authorId': '2542427', 'name': 'H. Palangi'},\n",
       "  {'authorId': '78846919', 'name': 'Marco Tulio Ribeiro'},\n",
       "  {'authorId': '144884116', 'name': 'Yi Zhang'}],\n",
       " [{'authorId': '2151645335', 'name': 'Ranu Sewada'},\n",
       "  {'authorId': '2279449118', 'name': 'Ashwani Jangid'},\n",
       "  {'authorId': '2279513475', 'name': 'Piyush Kumar'},\n",
       "  {'authorId': '2279448452', 'name': 'Neha Mishra'}],\n",
       " [{'authorId': '1379511816', 'name': 'Alejandro Barredo Arrieta'},\n",
       "  {'authorId': '2058921025', 'name': 'Natalia Díaz Rodríguez'},\n",
       "  {'authorId': '9221552', 'name': 'J. Ser'},\n",
       "  {'authorId': '1379511786', 'name': 'Adrien Bennetot'},\n",
       "  {'authorId': '3030006', 'name': 'S. Tabik'},\n",
       "  {'authorId': '50449165', 'name': 'A. Barbado'},\n",
       "  {'authorId': '39558258', 'name': 'S. García'},\n",
       "  {'authorId': '1402195255', 'name': 'S. Gil-Lopez'},\n",
       "  {'authorId': '145337392', 'name': 'D. Molina'},\n",
       "  {'authorId': '2445552', 'name': 'Richard Benjamins'},\n",
       "  {'authorId': '2091924780', 'name': 'Raja Chatila'},\n",
       "  {'authorId': '2098723448', 'name': 'Francisco Herrera'}]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sem_sel.authors.to_list()[:3] # nur die ersten 3 elemente der autoren als liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ranu Sewada', 'Ashwani Jangid', 'Piyush Kumar', 'Neha Mishra']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a [\"name\"] for a in df_sem_sel.authors.to_list()[1]] # listcomprehension: für jedes a in einer liste den namen ausgeben\n",
    "# diese form soll in feld ersatzweise gschrieben werden, dfür funktion schreiben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_authors(x):\n",
    "    return [a [\"name\"] for a in x] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ranu Sewada', 'Ashwani Jangid', 'Piyush Kumar', 'Neha Mishra']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_authors(df_sem_sel.authors.to_list()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sem_sel[\"authors\"] = df_sem_sel.apply(lambda row: clean_authors(row[\"authors\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_bib_sel, df_sem_sel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8279"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2021', '2020', '2019', '2018', '2023', '1994', '2022', '2017',\n",
       "       '2014', '1973', '2003', '1995', '2007', '2015', '1997', '2010',\n",
       "       '1985', '1992', '1990', '1999', '1993', '2000', '1987', '1991',\n",
       "       '2011', '1996', '2004', '2005', '2016', '2013', '1977', '1976',\n",
       "       '2002', '1982', '2012', '2008', '1989', '1984', '1988', '2009',\n",
       "       '1986', '1980', '1979', '2006', '1998', '1974', '1970', '1981',\n",
       "       '1975', '2024', '1963', '2001', '1961', '1962', '1983', '1958',\n",
       "       '1972', '1978', '1960'], dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"year\"] = df.apply(lambda row: str(row[\"year\"]).replace(\"{\",\"\").replace(\"}\",\"\"), axis = 1) # \"{\" wird entfernt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.year.astype(int) # hier würden die zahlen als integer umgewandelt werden, wenn df = .. davor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset = \"title\", keep = \"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tf_idf(query, docno, index):\n",
    "# alle terme aufsplitten und tf idef für alle terme einzeln berechnen, am ende addiert\n",
    "\n",
    "# alte suchmaschine nutzen um die reine tf zu berechnen und in tf-row abspeichern\n",
    "\n",
    "# get idf for term - funktion für idf nutzen\n",
    "\n",
    "# multiplikation für jeden term von tf mit idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc('machine learning', docno4, index_mult) # argument geüllt für die anwendung\n",
    "\n",
    "# diese cald- funktion muss thereotscih für ale dukomnete im korpus einzeln angewendet werden\n",
    "# mit der tf idf funktion von pyterrier wird es automatisch schon über alle dokumente angewendet, dort sieht man aber nicht richtig,\n",
    "# was dahinter geschieht"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
